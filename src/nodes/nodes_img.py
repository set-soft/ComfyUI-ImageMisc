# -*- coding: utf-8 -*-
# Copyright (c) 2025 Salvador E. Tropea
# Copyright (c) 2025 Instituto Nacional de TecnologÃ¯a Industrial
# License: GPL-3.0
# ImagePad, ImageResize are from Kijai (https://github.com/kijai/ComfyUI-KJNodes/)
# Project: ComfyUI-ImageMisc
# From code generated by Gemini 2.5 Pro
import numpy as np
import os
from PIL import Image  # Import the Python Imaging Library
from seconohe.apply_mask import apply_mask
from seconohe.foreground_estimation.affce import affce
from seconohe.foreground_estimation.fmlfe import fmlfe, IMPL_PRIORITY
from seconohe.color import color_to_rgb_float
from seconohe.downloader import download_file
from seconohe.color import color_to_rgb_uint8
# We are the main source, so we use the main_logger
from . import main_logger
import torch
import torch.nn.functional as F
from torchvision import transforms
import torchvision.transforms.functional as TF
from typing import Optional
try:
    from folder_paths import get_input_directory   # To get the ComfyUI input directory
    from comfy import model_management
    from comfy.utils import common_upscale
except ModuleNotFoundError:
    # No ComfyUI, this is a test environment
    def get_input_directory():
        return ""

try:
    from nodes import ImageScale
except Exception:
    class ImageScale(object):
        upscale_methods = ["nearest-exact", "bilinear", "area", "bicubic", "lanczos"]
try:
    from nodes import MAX_RESOLUTION
except Exception:
    MAX_RESOLUTION = 16384
try:
    from server import PromptServer
except ModuleNotFoundError:
    PromptServer = None
try:
    # We need to import the built-in LoadImage class for ImageDownload
    from nodes import LoadImage
    has_load_image = True
except Exception:
    has_load_image = False

logger = main_logger
BASE_CATEGORY = "image"
IO_CATEGORY = "io"
MANIPULATION_CATEGORY = "manipulation"
NORMALIZATION = "normalization"
FOREGROUND = "foreground"
BLUR_SIZE_OPT = ("INT", {"default": 90, "min": 1, "max": 255, "step": 1, })
BLUR_SIZE_TWO_OPT = ("INT", {"default": 6, "min": 1, "max": 255, "step": 1, })
COLOR_OPT = ("STRING", {
                "default": "#000000",
                "tooltip": "Color for fill.\n"
                           "Can be an hexadecimal (#RRGGBB).\n"
                           "Can comma separated RGB values in [0-255] or [0-1.0] range."})
DEFAULT_UPSCALE = 'bicubic' # transforms.InterpolationMode.BICUBIC.value
UPSCALE_OPT = (ImageScale.upscale_methods, { # [mode.value for mode in transforms.InterpolationMode]
                "default": DEFAULT_UPSCALE,
                "tooltip": "Interpolation method for image resize"
                })
PAD_SIZE_OPT = ("INT", {"default": 0, "min": 0, "max": MAX_RESOLUTION, "step": 1, })
SIZE_OPT = ("INT", {"default": 512, "min": 0, "max": MAX_RESOLUTION, "step": 1, })
SIZE_OPT_FI = tuple(SIZE_OPT)
SIZE_OPT_FI[1]["forceInput"] = True
MASK_UPSCALE = 'nearest-exact'  # transforms.InterpolationMode.NEAREST_EXACT.value
BEST_UPSCALE = 'lanczos'        # transforms.InterpolationMode.LANCZOS.value


def tensor_to_pil(tensor: torch.Tensor) -> Image.Image:
    """Converts a single image tensor (H, W, C) [0, 1] to a Pillow Image."""
    np_image = (tensor.cpu().numpy() * 255).astype(np.uint8)
    return Image.fromarray(np_image)


def pil_to_tensor(pil_image: Image.Image) -> torch.Tensor:
    """Converts a Pillow Image to a tensor (H, W, C) [0, 1]."""
    np_image = np.array(pil_image).astype(np.float32) / 255.0
    return torch.from_numpy(np_image)


def upscale(image, width, height, upscale_method):
    # return F.interpolate(image, size=(height, width), mode=upscale_method)
    return common_upscale(image, width, height, upscale_method, crop="disabled")


if has_load_image:
    class ImageDownload:
        @classmethod
        def INPUT_TYPES(cls):
            return {
                "required": {
                    "base_url": ("STRING", {
                        "default":
                            "https://raw.githubusercontent.com/set-soft/AudioSeparation/refs/heads/main/example_workflows/",
                        "tooltip": "The base URL where the image file is located."
                    }),
                    "filename": ("STRING", {
                        "default": "audioseparation_logo.jpg",
                        "tooltip": "The name of the image file to download (e.g., photo.jpg, art.png)."
                    }),
                },
                "optional": {
                    "image_bypass": ("IMAGE", {
                         "tooltip": "If this image is present will be used instead of the downloaded one"
                    }),
                    "mask_bypass": ("MASK", {"tooltip": "If this mask is present will be used instead of the downloaded one"}),
                    "local_name": ("STRING", {
                        "default": "",
                        "tooltip": "The name used locally. Leave empty to use `filename`"
                    }),
                }
            }

        RETURN_TYPES = ("IMAGE", "MASK")
        RETURN_NAMES = ("image", "alpha_mask")
        FUNCTION = "load_or_download_image"
        CATEGORY = BASE_CATEGORY + "/" + IO_CATEGORY
        DESCRIPTION = ("Downloads an image to ComfyUI's 'input' directory if it doesn't exist, then loads it using the "
                       "built-in LoadImage logic.")
        UNIQUE_NAME = "SET_ImageDownload"
        DISPLAY_NAME = "Image Download and Load"
        # This node stores a result to disk. So this IS an output node.
        # It can be used without connecting any other node.
        # Declaring it as output helps with the preview mechanism.
        OUTPUT_NODE = True

        def load_or_download_image(self, base_url: str, filename: str, image_bypass: Optional[torch.Tensor] = None,
                                   mask_bypass: Optional[torch.Tensor] = None, local_name: str = None):
            # If we have something at the bypass inputs use it
            if image_bypass is not None or mask_bypass is not None:
                if image_bypass is None:
                    # Just a mask
                    assert mask_bypass is not None, "This should not be possible if image_bypass is None"  # For mypy
                    image_bypass = torch.zeros(mask_bypass.shape + (3,), dtype=torch.float32, device="cpu")
                    logger.warning("ImageDownload: Returning an empty image")
                elif mask_bypass is None:
                    # This is ComfyUI behavior when we don't have transparency
                    mask_bypass = torch.zeros((64, 64), dtype=torch.float32, device="cpu").unsqueeze(0)
                    logger.warning("ImageDownload: Returning an empty mask")
                return (image_bypass, mask_bypass)

            save_dir = get_input_directory()
            dest_fname = local_name or filename
            local_filepath = os.path.join(save_dir, dest_fname)

            if not os.path.exists(local_filepath):
                logger.info(f"File '{filename}' not found locally. Attempting to download.")

                if not base_url.endswith('/'):
                    base_url += '/'
                download_url = base_url + filename

                try:
                    download_file(logger, url=download_url, save_dir=save_dir, file_name=dest_fname, kind="image")
                except Exception as e:
                    logger.error(f"Download failed for {download_url}: {e}", exc_info=True)
                    raise
            else:
                logger.info(f"Found existing file, skipping download: '{local_filepath}'")

            # --- REUSE ComfyUI's LoadImage LOGIC ---
            try:
                # Instantiate the built-in LoadImage node
                loader_instance = LoadImage()

                # The LoadImage node's `load_image` method expects the filename as passed
                # by the ComfyUI widget, which is just the filename. It internally
                # resolves the path using folder_paths.

                logger.debug(f"Calling built-in LoadImage.load_image() with filename: '{dest_fname}'")

                # Call the method and return its result directly
                result = loader_instance.load_image(dest_fname)
                # This information is for the preview, as we are an output node and we return images
                # they will be displayed in our node. Quite simple.
                downloaded_file = {
                     "images": [{
                         "filename": dest_fname,
                         "subfolder": "",
                         "type": "input"  # We stored the file in the "input" folder
                     }]
                }
                return {"ui": downloaded_file, "result": result}

            except Exception as e:
                logger.error(f"Failed to load image '{filename}' using built-in LoadImage node: {e}", exc_info=True)
                # Re-raise to make the error visible in ComfyUI
                raise IOError(f"Could not load the image file '{filename}' using the standard loader. "
                              "It may be corrupt or in an unsupported format.") from e
else:
    logger.error("Failed to import ComfyUI `LoadImage`, please fill an issue here: "
                 "https://github.com/set-soft/ComfyUI-ImageMisc/issues")


class CompositeFace:
    """
    A ComfyUI node to composite (paste) animated face crops back onto reference images.
    It handles a M-to-N relationship, where M reference images and bboxes correspond
    to M*N animated face images.
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "animated": ("IMAGE",),      # The M*N batch of cropped faces
                "reference": ("IMAGE",),     # The M batch of original context images
                "bboxes": ("BBOX",),         # The M list of (x, y, w, h) tuples
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("images",)
    FUNCTION = "composite"

    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = ("Inserts the `animated` face in the `reference` images at the `bboxes` coordinates.")
    UNIQUE_NAME = "SET_CompositeFace"
    DISPLAY_NAME = "Face Composite"

    def composite(self, animated: torch.Tensor, reference: torch.Tensor, bboxes: list):
        # 1. Get batch sizes and validate the M vs M*N relationship
        ref_count = reference.shape[0]
        anim_count = animated.shape[0]
        bbox_count = len(bboxes)

        if ref_count == 0 or anim_count == 0:
            logger.info("Warning: One of the input image batches is empty. Returning empty tensor.")
            return (torch.zeros((0, 1, 1, 3)),)

        if ref_count != bbox_count:
            raise ValueError(f"Mismatch: Received {ref_count} reference images but {bbox_count} bboxes. "
                             "These must be equal.")

        if anim_count % ref_count != 0:
            raise ValueError(f"Batch size mismatch: The 'animated' batch ({anim_count}) is not a multiple of the "
                             f"'reference' batch ({ref_count}).")

        # N: Number of animated frames per reference image
        n_frames_per_ref = anim_count // ref_count
        logger.info(f"Processing {ref_count} reference images, each with {n_frames_per_ref} animated frames.")

        output_images = []

        # 2. Iterate through each reference image and its corresponding bbox
        for i in range(ref_count):
            ref_tensor = reference[i]
            bbox = bboxes[i]

            # The bbox from your code is the area to be replaced.
            # Assuming it's in the format (x, y, width, height)
            try:
                x, y, w, h = map(int, bbox)
            except (ValueError, TypeError) as e:
                raise TypeError(f"Bbox item {i} has an invalid format: {bbox}. Expected (x, y, w, h). Error: {e}")

            # Convert the reference image to Pillow Image ONCE before the inner loop
            ref_pil = tensor_to_pil(ref_tensor)

            # 3. For this one reference, loop through its N animated frames
            for j in range(n_frames_per_ref):
                anim_index = i * n_frames_per_ref + j
                anim_tensor = animated[anim_index]

                # Convert the small animated face to Pillow Image
                anim_pil = tensor_to_pil(anim_tensor)

                # 4. Resize the animated face to fit the target bbox
                # Image.Resampling.LANCZOS is a high-quality resampling filter.
                resized_anim_face = anim_pil.resize((w, h), Image.Resampling.LANCZOS)

                # 5. Perform the paste operation.
                # It's CRITICAL to work on a copy of the reference image for each frame.
                pasted_image_pil = ref_pil.copy()
                pasted_image_pil.paste(resized_anim_face, (x, y))

                # 6. Convert the final image back to a tensor and add to the output list
                final_tensor = pil_to_tensor(pasted_image_pil)
                output_images.append(final_tensor)

        # 7. Stack all the generated images into a single batch tensor
        if not output_images:
            return (torch.zeros_like(reference),)  # Return something if all pastes failed

        final_batch = torch.stack(output_images)

        return (final_batch,)


class CompositeFaceFrameByFrame(CompositeFace):
    """
    A ComfyUI node to composite animated frames onto reference frames on a 1-to-1 basis.
    It expects the 'animated' and 'reference' batches to have the same number of frames.
    It uses the *first* bounding box from the 'bboxes' input for all frames.
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "animated": ("IMAGE",),      # The batch of cropped/processed frames
                "reference": ("IMAGE",),     # The batch of original frames
                "bboxes": ("BBOX",),         # A list of bboxes; only the first is used
            },
        }

    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = ("Inserts the `animated` face in the `reference` video at the `bboxes` coordinates.")
    UNIQUE_NAME = "SET_CompositeFaceFrameByFrame"
    DISPLAY_NAME = "Face Composite (frame by frame)"

    def composite(self, animated: torch.Tensor, reference: torch.Tensor, bboxes: list):
        # 1. Validate inputs
        anim_count = animated.shape[0]
        ref_count = reference.shape[0]

        if anim_count != ref_count:
            raise ValueError(f"Batch size mismatch: Received {anim_count} animated frames and {ref_count} reference "
                             "frames. They must be equal.")

        if not bboxes:
            raise ValueError("Bboxes input is empty. A bounding box is required.")

        # 2. Extract the single bounding box to be used for all frames
        if len(bboxes) > 1:
            logger.info(f"Warning: Received {len(bboxes)} bboxes. Using only the first one for all frames.")

        try:
            # Use the first bbox from the list
            x, y, w, h = map(int, bboxes[0])
            bbox_to_use = (x, y, w, h)
        except (ValueError, TypeError) as e:
            raise TypeError(f"The first bbox has an invalid format: {bboxes[0]}. Expected (x, y, w, h). Error: {e}")

        logger.info(f"Compositing {anim_count} frames using static bbox: {bbox_to_use}")

        output_images = []

        # 3. Loop through each frame in a 1-to-1 fashion
        for i in range(anim_count):
            ref_tensor = reference[i]
            anim_tensor = animated[i]

            # Convert tensors to Pillow Images
            ref_pil = tensor_to_pil(ref_tensor)
            anim_pil = tensor_to_pil(anim_tensor)

            # 4. Resize the animated face to fit the bbox
            # Image.Resampling.LANCZOS is a high-quality filter comparable to OpenCV's INTER_CUBIC/LANCZOS4
            resized_anim_face = anim_pil.resize((w, h), Image.Resampling.LANCZOS)

            # 5. Perform the paste operation
            # Pillow's paste is simpler. It handles coordinates and requires a copy.
            pasted_image_pil = ref_pil.copy()
            pasted_image_pil.paste(resized_anim_face, (x, y))

            # 6. Convert back to tensor and add to output list
            final_tensor = pil_to_tensor(pasted_image_pil)
            output_images.append(final_tensor)

        # 7. Stack all images into the final output batch
        final_batch = torch.stack(output_images)

        return (final_batch,)


class NormalizeToImageNetDataset():
    """
    A ComfyUI node to normalize the values to the mean/std of the ImageNet dataset
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
            },
        }
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "normalize"
    CATEGORY = BASE_CATEGORY + "/" + NORMALIZATION
    DESCRIPTION = ("Normalize the image to the ImageNet dataset")
    UNIQUE_NAME = "SET_NormalizeToImageNetDataset"
    DISPLAY_NAME = "Normalize Image to ImageNet"
    imagenet_normalize = None

    def normalize(self, image: torch.Tensor):
        return (TF.normalize(image.permute(0, 3, 1, 2),  # BHWC -> BCHW
                             mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225]).permute(0, 2, 3, 1),)  # BCHW -> BHWC


class NormalizeToRangeMinus05to05():
    """
    A ComfyUI node to normalize the values to the [-0.5, 0.5] range
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {"required": {"image": ("IMAGE",), }, }
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "normalize"
    CATEGORY = BASE_CATEGORY + "/" + NORMALIZATION
    DESCRIPTION = ("Normalize the image to [-0.5, 0.5]")
    UNIQUE_NAME = "SET_NormalizeToRangeMinus05to05"
    DISPLAY_NAME = "Normalize Image to [-0.5, 0.5]"
    imagenet_normalize = None

    def normalize(self, image: torch.Tensor):
        return (TF.normalize(image.permute(0, 3, 1, 2),  # BHWC -> BCHW
                             mean=[0.5, 0.5, 0.5],
                             std=[1.0, 1.0, 1.0]).permute(0, 2, 3, 1),)  # BCHW -> BHWC


class NormalizeToRangeMinus1to1():
    """
    A ComfyUI node to normalize the values to the [-1, 1] range
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {"required": {"image": ("IMAGE",), }, }
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "normalize"
    CATEGORY = BASE_CATEGORY + "/" + NORMALIZATION
    DESCRIPTION = ("Normalize the image to [-1, 1]")
    UNIQUE_NAME = "SET_NormalizeToRangeMinus1to1"
    DISPLAY_NAME = "Normalize Image to [-1, 1] (i.e. GAN)"
    imagenet_normalize = None

    def normalize(self, image: torch.Tensor):
        return (TF.normalize(image.permute(0, 3, 1, 2),  # BHWC -> BCHW
                             mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5]).permute(0, 2, 3, 1),)  # BCHW -> BHWC


class ApplyMaskAFFCE:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "images": ("IMAGE",),
                "masks": ("MASK",),
                "blur_size": BLUR_SIZE_OPT,
                "blur_size_two": BLUR_SIZE_TWO_OPT,
                "fill_color": ("BOOLEAN", {
                    "default": False,
                    "tooltip": ("Fill the background using a color.\n"
                                "Returns an RGB image, otherwise an RGBA.")
                }),
                "color": COLOR_OPT,
                "batched":  ("BOOLEAN", {
                    "default": True,
                    "tooltip": ("Process the images at once.\n"
                                "Faster, needs more memory")
                }),
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK",)
    RETURN_NAMES = ("image", "mask",)
    FUNCTION = "get_foreground"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = ("Apply a mask to an image using\n"
                   "Approximate Fast Foreground Colour Estimation.\n"
                   "https://github.com/Photoroom/fast-foreground-estimation")
    UNIQUE_NAME = "SET_ApplyMaskAFFCE"
    DISPLAY_NAME = "Apply Mask using AFFCE"

    def get_foreground(self, images, masks, blur_size=91, blur_size_two=7, fill_color=False, color=None, batched=True):
        out_images = apply_mask(logger, images, masks, model_management.get_torch_device(), blur_size, blur_size_two,
                                fill_color, color, batched)
        return out_images.cpu(), masks.cpu()


class AFFCE:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "images": ("IMAGE",),
                "masks": ("MASK",),
                "blur_size": BLUR_SIZE_OPT,
                "blur_size_two": BLUR_SIZE_TWO_OPT,
                "batched":  ("BOOLEAN", {
                    "default": True,
                    "tooltip": ("Process the images at once.\n"
                                "Faster, needs more memory")
                }),
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK",)
    RETURN_NAMES = ("foreground", "mask",)
    FUNCTION = "get_foreground"
    CATEGORY = BASE_CATEGORY + "/" + FOREGROUND
    DESCRIPTION = ("Estimate the foreground image using\n"
                   "Approximate Fast Foreground Colour Estimation.\n"
                   "https://github.com/Photoroom/fast-foreground-estimation")
    UNIQUE_NAME = "SET_AFFCE"
    DISPLAY_NAME = "Estimate foreground (AFFCE)"

    def get_foreground(self, images, masks, blur_size=91, blur_size_two=7, batched=True):
        device = model_management.get_torch_device()
        images_on_device = images.to(device)
        masks_on_device = masks.to(device)

        out_images = affce(images_on_device, masks_on_device, r1=blur_size, r2=blur_size_two, batched=batched)

        return out_images.cpu(), masks.cpu()


class FMLFE:
    """
    A ComfyUI node that uses the Fast Multi-Level Foreground Estimation algorithm
    to produce a high-quality foreground and background separation. It can
    intelligently select the best available backend (CuPy, OpenCL, Numba, or PyTorch).
    """

    @classmethod
    def INPUT_TYPES(cls):
        # Create the dropdown list for the implementation choice
        impl_list = ['auto'] + IMPL_PRIORITY

        return {
            "required": {
                "images": ("IMAGE", {
                    "tooltip": "The source image(s) from which to estimate the foreground and background."
                }),
                "masks": ("MASK", {
                    "tooltip": "The alpha matte that guides the estimation. White areas are treated as known "
                               "foreground, black as known background, and gray areas are the semi-transparent "
                               "regions the algorithm will solve for."
                }),
                "implementation": (impl_list, {
                    "default": "auto",
                    "tootip": "Select the computation backend. 'auto' mode will automatically try to use the "
                              "fastest available implementation, in order of priority: CuPy (NVIDIA GPU), "
                              "OpenCL (GPU), Numba (CPU/GPU), and finally the pure PyTorch version."
                }),
            },
            "optional": {
                "regularization": ("FLOAT", {
                    "default": 1e-5,
                    "min": 0.0,
                    "max": 0.1,
                    "step": 1e-5,
                    "display": "number",
                    "tooltip": "The regularization strength (epsilon). This acts as a smoothness prior. "
                               "Higher values result in smoother, more blended foreground and background colors, "
                               "but may lose very fine details. Lower values preserve more detail but can be noisier."
                }),
                "n_small_iterations": ("INT", {
                    "default": 10,
                    "min": 1,
                    "max": 100,
                    "tooltip": "The number of solver iterations to perform on the lower-resolution levels of the "
                               "image pyramid. More iterations can improve quality at the cost of speed."
                }),
                "n_big_iterations": ("INT", {
                    "default": 2,
                    "min": 1,
                    "max": 100,
                    "tooltip": "The number of solver iterations to perform on the higher-resolution (larger) levels "
                               "of the image pyramid. Fewer iterations are typically needed at high resolution as the "
                               "details are propagated up from the smaller levels."
                }),
                "small_size": ("INT", {
                    "default": 32,
                    "min": 8,
                    "max": 256,
                    "tooltip": "The pixel dimension threshold. Image pyramid levels smaller than this size will use "
                               "the higher 'n_small_iterations' count, while larger levels will use 'n_big_iterations'."
                }),
                "gradient_weight": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 10.0,
                    "step": 0.1,
                    "tooltip": "Controls how strongly the edges in the alpha matte influence color blending. "
                               "A higher value makes the algorithm respect the mask's edges more, leading to sharper "
                               "color boundaries. A lower value allows more color bleeding, an effect similar to "
                               "increasing regularization."
                }),
            }
        }

    RETURN_TYPES = ("IMAGE", "IMAGE", "MASK",)
    RETURN_NAMES = ("foreground", "background", "mask")
    FUNCTION = "estimate"
    CATEGORY = BASE_CATEGORY + "/" + FOREGROUND
    DESCRIPTION = ("Estimate the foreground image using\n"
                   "Fast Multi-Level Foreground Estimation.")
    UNIQUE_NAME = "SET_FMLFE"
    DISPLAY_NAME = "Estimate foreground (FMLFE)"

    def estimate(self, images: torch.Tensor, masks: torch.Tensor, implementation: str,
                 regularization: float, n_small_iterations: int, n_big_iterations: int,
                 small_size: int, gradient_weight: float):
        try:
            foregrounds, backgrounds = fmlfe(
                images=images,
                masks=masks,
                logger=logger,
                implementation=implementation,
                regularization=regularization,
                n_small_iterations=n_small_iterations,
                n_big_iterations=n_big_iterations,
                small_size=small_size,
                gradient_weight=gradient_weight
            )

            return (foregrounds, backgrounds, masks,)

        except Exception as e:
            # This ensures that if all backends fail, the error is clearly visible in the ComfyUI console.
            logger.error("Failed to execute ML Foreground Estimation. All backends failed.")
            logger.error(f"Last error: {e}")
            # Raising the exception will stop the workflow and show the error to the user.
            raise e


class CreateEmptyImage:
    """
    A ComfyUI node to create a solid-color image tensor.
    The output dimensions can be specified manually or inherited from an optional input image.
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "width": ("INT", {
                    "default": 1024,
                    "min": 1,
                    "max": 8192,
                    "step": 8,
                    "tooltip": "The width of the new image in pixels. This value is ignored if a `reference` is provided."
                }),
                "height": ("INT", {
                    "default": 1024,
                    "min": 1,
                    "max": 8192,
                    "step": 8,
                    "tooltip": "The height of the new image in pixels. This value is ignored if a `reference` is provided."
                }),
                "batch_size": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 64,
                    "tooltip": "The number of images to create in the batch. This value is ignored if a `reference` "
                               "is provided."
                }),
                "color": COLOR_OPT,
            },
            "optional": {
                "reference": ("IMAGE", {
                    "tooltip": "If an image is connected here, its dimensions (batch size, height, and width) will be "
                               "used for the new image, overriding the manual width, height, and batch_size inputs."
                }),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "create_image"
    CATEGORY = BASE_CATEGORY + "/generation"
    DESCRIPTION = ("Create a solid-color image.\n"
                   "If the optional image is provides uses its shape.")
    UNIQUE_NAME = "SET_CreateEmptyImage"
    DISPLAY_NAME = "Create Empty Image"

    def create_image(self, width: int, height: int, batch_size: int, color: str,
                     reference: Optional[torch.Tensor] = None):
        # --- 1. Determine the final shape of the output tensor ---
        if reference is not None:
            # If an image is provided, its shape overrides the manual inputs
            b, h, w, _ = reference.shape
        else:
            b, h, w = batch_size, height, width

        # --- 2. Parse the color string ---
        # The function returns a tuple of floats in the [0, 1] range
        rgb_color = color_to_rgb_float(logger, color)

        # --- 3. Create the tensor efficiently ---
        # Create a small color tensor and then expand it to the final size.
        # This is highly memory-efficient as it creates a view, not a full-size copy.
        # Tensors should be created on the CPU by default in generator nodes.
        color_tensor = torch.tensor(rgb_color, dtype=torch.float32, device="cpu").view(1, 1, 1, 3)
        final_image = color_tensor.expand(b, h, w, 3)

        return (final_image,)


# Adapted from KJNodes, credits to Kijai
class ImagePad:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE", ),
                "left": PAD_SIZE_OPT,
                "right": PAD_SIZE_OPT,
                "top": PAD_SIZE_OPT,
                "bottom": PAD_SIZE_OPT,
                "extra_padding": PAD_SIZE_OPT,
                "pad_mode": (["edge", "color"],),
                "color": COLOR_OPT,
            },
            "optional": {
                "mask": ("MASK", ),
                "target_width": SIZE_OPT_FI,
                "target_height": SIZE_OPT_FI,
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK", )
    RETURN_NAMES = ("images", "masks",)
    FUNCTION = "pad"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = "Pad the input image and optionally mask with the specified padding."
    UNIQUE_NAME = "SET_ImagePad"
    DISPLAY_NAME = "Pad Image (KJ/SET)"

    def pad(self, image, left, right, top, bottom, extra_padding, color, pad_mode, mask=None, target_width=None,
            target_height=None):
        B, H, W, C = image.shape

        # Resize masks to image dimensions if necessary
        if mask is not None:
            BM, HM, WM = mask.shape
            if HM != H or WM != W:
                mask = F.interpolate(mask.unsqueeze(1), size=(H, W), mode=MASK_UPSCALE).squeeze(1)

        # Parse background color
        bg_color = torch.tensor(color_to_rgb_uint8(logger, color), dtype=image.dtype, device=image.device)

        # Calculate padding sizes with extra padding
        if target_width is not None and target_height is not None:
            if extra_padding > 0:
                image = upscale(image.movedim(-1, 1), W - extra_padding, H - extra_padding, BEST_UPSCALE).movedim(1, -1)
                B, H, W, C = image.shape

            padded_width = target_width
            padded_height = target_height
            pad_left = (padded_width - W) // 2
            pad_right = padded_width - W - pad_left
            pad_top = (padded_height - H) // 2
            pad_bottom = padded_height - H - pad_top
        else:
            pad_left = left + extra_padding
            pad_right = right + extra_padding
            pad_top = top + extra_padding
            pad_bottom = bottom + extra_padding

            padded_width = W + pad_left + pad_right
            padded_height = H + pad_top + pad_bottom
        out_image = torch.zeros((B, padded_height, padded_width, C), dtype=image.dtype, device=image.device)

        # Fill padded areas
        for b in range(B):
            if pad_mode == "edge":
                # Pad with edge color
                # Define edge pixels
                top_edge = image[b, 0, :, :]
                bottom_edge = image[b, H-1, :, :]
                left_edge = image[b, :, 0, :]
                right_edge = image[b, :, W-1, :]

                # Fill borders with edge colors
                out_image[b, :pad_top, :, :] = top_edge.mean(dim=0)
                out_image[b, pad_top+H:, :, :] = bottom_edge.mean(dim=0)
                out_image[b, :, :pad_left, :] = left_edge.mean(dim=0)
                out_image[b, :, pad_left+W:, :] = right_edge.mean(dim=0)
                out_image[b, pad_top:pad_top+H, pad_left:pad_left+W, :] = image[b]
            else:
                # Pad with specified background color
                out_image[b, :, :, :] = bg_color.unsqueeze(0).unsqueeze(0)  # Expand for H and W dimensions
                out_image[b, pad_top:pad_top+H, pad_left:pad_left+W, :] = image[b]

        if mask is not None:
            out_masks = torch.nn.functional.pad(
                mask,
                (pad_left, pad_right, pad_top, pad_bottom),
                mode='replicate'
            )
        else:
            out_masks = torch.ones((B, padded_height, padded_width), dtype=image.dtype, device=image.device)
            for m in range(B):
                out_masks[m, pad_top:pad_top+H, pad_left:pad_left+W] = 0.0

        return (out_image, out_masks)


# Adapted from KJNodes, credits to Kijai
class ImageResize:
    """
    A resize and crop node, from ImageResizeKJv2
    """
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE",),
                "width": SIZE_OPT,
                "height": SIZE_OPT,
                "upscale_method": UPSCALE_OPT,
                "keep_proportion": (["stretch", "resize", "pad", "pad_edge", "crop"], {"default": False}),
                "pad_color": COLOR_OPT,
                "crop_position": (["center", "top", "bottom", "left", "right"], {"default": "center"}),
                "divisible_by": ("INT", {"default": 2, "min": 0, "max": 512, "step": 1}),
            },
            "optional": {
                "mask": ("MASK",),
                "device": (["cpu", "gpu"],),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
            },
        }

    RETURN_TYPES = ("IMAGE", "INT", "INT", "MASK",)
    RETURN_NAMES = ("IMAGE", "width", "height", "mask",)
    FUNCTION = "resize"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = ("Resizes the image to the specified width and height.\n"
                   "Size can be retrieved from the input.\n\n"
                   "Keep proportions keeps the aspect ratio of the image, by\n"
                   "highest dimension.")
    UNIQUE_NAME = "SET_ImageResize"
    DISPLAY_NAME = "Resize Image (KJ/SET)"

    def resize(self, image, width, height, keep_proportion, upscale_method, divisible_by, pad_color, crop_position,
               unique_id, device="cpu", mask=None):
        B, H, W, C = image.shape

        if device == "gpu":
            if upscale_method == "lanczos":
                raise Exception("Lanczos is not supported on the GPU")
            device = model_management.get_torch_device()
        else:
            device = torch.device("cpu")

        if width == 0:
            width = W
        if height == 0:
            height = H

        if keep_proportion == "resize" or keep_proportion.startswith("pad"):
            # If one of the dimensions is zero, calculate it to maintain the aspect ratio
            if width == 0 and height != 0:
                ratio = height / H
                new_width = round(W * ratio)
            elif height == 0 and width != 0:
                ratio = width / W
                new_height = round(H * ratio)
            elif width != 0 and height != 0:
                # Scale based on which dimension is smaller in proportion to the desired dimensions
                ratio = min(width / W, height / H)
                new_width = round(W * ratio)
                new_height = round(H * ratio)

            if keep_proportion.startswith("pad"):
                # Calculate padding based on position
                if crop_position == "center":
                    pad_left = (width - new_width) // 2
                    pad_right = width - new_width - pad_left
                    pad_top = (height - new_height) // 2
                    pad_bottom = height - new_height - pad_top
                elif crop_position == "top":
                    pad_left = (width - new_width) // 2
                    pad_right = width - new_width - pad_left
                    pad_top = 0
                    pad_bottom = height - new_height
                elif crop_position == "bottom":
                    pad_left = (width - new_width) // 2
                    pad_right = width - new_width - pad_left
                    pad_top = height - new_height
                    pad_bottom = 0
                elif crop_position == "left":
                    pad_left = 0
                    pad_right = width - new_width
                    pad_top = (height - new_height) // 2
                    pad_bottom = height - new_height - pad_top
                elif crop_position == "right":
                    pad_left = width - new_width
                    pad_right = 0
                    pad_top = (height - new_height) // 2
                    pad_bottom = height - new_height - pad_top

            width = new_width
            height = new_height

        if divisible_by > 1:
            width = width - (width % divisible_by)
            height = height - (height % divisible_by)

        out_image = image.clone().to(device)

        if mask is not None:
            out_mask = mask.clone().to(device)
        else:
            out_mask = None

        if keep_proportion == "crop":
            old_width = W
            old_height = H
            old_aspect = old_width / old_height
            new_aspect = width / height

            # Calculate dimensions to keep
            if old_aspect > new_aspect:  # Image is wider than target
                crop_w = round(old_height * new_aspect)
                crop_h = old_height
            else:  # Image is taller than target
                crop_w = old_width
                crop_h = round(old_width / new_aspect)

            # Calculate crop position
            if crop_position == "center":
                x = (old_width - crop_w) // 2
                y = (old_height - crop_h) // 2
            elif crop_position == "top":
                x = (old_width - crop_w) // 2
                y = 0
            elif crop_position == "bottom":
                x = (old_width - crop_w) // 2
                y = old_height - crop_h
            elif crop_position == "left":
                x = 0
                y = (old_height - crop_h) // 2
            elif crop_position == "right":
                x = old_width - crop_w
                y = (old_height - crop_h) // 2

            # Apply crop
            out_image = out_image.narrow(-2, x, crop_w).narrow(-3, y, crop_h)
            if mask is not None:
                out_mask = out_mask.narrow(-1, x, crop_w).narrow(-2, y, crop_h)

        out_image = upscale(out_image.movedim(-1, 1), width, height, upscale_method).movedim(1, -1)

        if mask is not None:
            # if upscale_method == "lanczos":
            #    out_mask = upscale(out_mask.unsqueeze(1).repeat(1, 3, 1, 1), width, height,
            #                        upscale_method).movedim(1, -1)[:, :, :, 0]
            # else:
            out_mask = upscale(out_mask.unsqueeze(1), width, height, upscale_method).squeeze(1)

        if keep_proportion.startswith("pad"):
            if pad_left > 0 or pad_right > 0 or pad_top > 0 or pad_bottom > 0:
                padded_width = width + pad_left + pad_right
                padded_height = height + pad_top + pad_bottom
                if divisible_by > 1:
                    width_remainder = padded_width % divisible_by
                    height_remainder = padded_height % divisible_by
                    if width_remainder > 0:
                        extra_width = divisible_by - width_remainder
                        pad_right += extra_width
                    if height_remainder > 0:
                        extra_height = divisible_by - height_remainder
                        pad_bottom += extra_height
                out_image, _ = ImagePad.pad(self, out_image, pad_left, pad_right, pad_top, pad_bottom, 0, pad_color,
                                            "edge" if keep_proportion == "pad_edge" else "color")
                if mask is not None:
                    out_mask = out_mask.unsqueeze(1).repeat(1, 3, 1, 1).movedim(1, -1)
                    out_mask, _ = ImagePad.pad(self, out_mask, pad_left, pad_right, pad_top, pad_bottom, 0, pad_color,
                                               "edge" if keep_proportion == "pad_edge" else "color")
                    out_mask = out_mask[:, :, :, 0]
                else:
                    B, H_pad, W_pad, _ = out_image.shape
                    out_mask = torch.ones((B, H_pad, W_pad), dtype=out_image.dtype, device=out_image.device)
                    out_mask[:, pad_top:pad_top+height, pad_left:pad_left+width] = 0.0

        if unique_id and PromptServer is not None:
            try:
                num_elements = out_image.numel()
                element_size = out_image.element_size()
                memory_size_mb = (num_elements * element_size) / (1024 * 1024)

                PromptServer.instance.send_progress_text(
                    f"<tr><td>Output: </td><td><b>{out_image.shape[0]}</b> x <b>{out_image.shape[2]}</b> x <b>"
                    f"{out_image.shape[1]} | {memory_size_mb:.2f}MB</b></td></tr>",
                    unique_id
                )
            except Exception:
                pass

        return (out_image.cpu(), out_image.shape[2], out_image.shape[1],
                out_mask.cpu() if out_mask is not None else
                torch.zeros(64, 64, device=torch.device("cpu"), dtype=torch.float32))
