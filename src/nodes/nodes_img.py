# -*- coding: utf-8 -*-
# Copyright (c) 2025 Salvador E. Tropea
# Copyright (c) 2025 Instituto Nacional de TecnologÃ¯a Industrial
# License: GPL-3.0
# Project: ComfyUI-ImageMisc
# From code generated by Gemini 2.5 Pro
import numpy as np
import os
from PIL import Image  # Import the Python Imaging Library
from seconohe.apply_mask import apply_mask
from seconohe.foreground_estimation.affce import affce
from seconohe.foreground_estimation.fmlfe import fmlfe, IMPL_PRIORITY
from seconohe.color import color_to_rgb_float
from seconohe.downloader import download_file
# We are the main source, so we use the main_logger
from . import main_logger
import torch
import torchvision.transforms.functional as TF
from typing import Optional
try:
    from folder_paths import get_input_directory   # To get the ComfyUI input directory
    from comfy import model_management
except ModuleNotFoundError:
    # No ComfyUI, this is a test environment
    def get_input_directory():
        return ""
try:
    # We need to import the built-in LoadImage class for ImageDownload
    from nodes import LoadImage
    has_load_image = True
except Exception:
    has_load_image = False

logger = main_logger
BASE_CATEGORY = "image"
IO_CATEGORY = "io"
MANIPULATION_CATEGORY = "manipulation"
NORMALIZATION = "normalization"
FOREGROUND = "foreground"
BLUR_SIZE_OPT = ("INT", {"default": 90, "min": 1, "max": 255, "step": 1, })
BLUR_SIZE_TWO_OPT = ("INT", {"default": 6, "min": 1, "max": 255, "step": 1, })
COLOR_OPT = ("STRING", {
                "default": "#000000",
                "tooltip": "Color for fill.\n"
                           "Can be an hexadecimal (#RRGGBB).\n"
                           "Can comma separated RGB values in [0-255] or [0-1.0] range."})


def tensor_to_pil(tensor: torch.Tensor) -> Image.Image:
    """Converts a single image tensor (H, W, C) [0, 1] to a Pillow Image."""
    np_image = (tensor.cpu().numpy() * 255).astype(np.uint8)
    return Image.fromarray(np_image)


def pil_to_tensor(pil_image: Image.Image) -> torch.Tensor:
    """Converts a Pillow Image to a tensor (H, W, C) [0, 1]."""
    np_image = np.array(pil_image).astype(np.float32) / 255.0
    return torch.from_numpy(np_image)


if has_load_image:
    class ImageDownload:
        @classmethod
        def INPUT_TYPES(cls):
            return {
                "required": {
                    "base_url": ("STRING", {
                        "default":
                            "https://raw.githubusercontent.com/set-soft/AudioSeparation/refs/heads/main/example_workflows/",
                        "tooltip": "The base URL where the image file is located."
                    }),
                    "filename": ("STRING", {
                        "default": "audioseparation_logo.jpg",
                        "tooltip": "The name of the image file to download (e.g., photo.jpg, art.png)."
                    }),
                },
                "optional": {
                    "image_bypass": ("IMAGE", {
                         "tooltip": "If this image is present will be used instead of the downloaded one"
                    }),
                    "mask_bypass": ("MASK", {"tooltip": "If this mask is present will be used instead of the downloaded one"}),
                    "local_name": ("STRING", {
                        "default": "",
                        "tooltip": "The name used locally. Leave empty to use `filename`"
                    }),
                }
            }

        RETURN_TYPES = ("IMAGE", "MASK")
        RETURN_NAMES = ("image", "alpha_mask")
        FUNCTION = "load_or_download_image"
        CATEGORY = BASE_CATEGORY + "/" + IO_CATEGORY
        DESCRIPTION = ("Downloads an image to ComfyUI's 'input' directory if it doesn't exist, then loads it using the "
                       "built-in LoadImage logic.")
        UNIQUE_NAME = "SET_ImageDownload"
        DISPLAY_NAME = "Image Download and Load"
        # This node stores a result to disk. So this IS an output node.
        # It can be used without connecting any other node.
        # Declaring it as output helps with the preview mechanism.
        OUTPUT_NODE = True

        def load_or_download_image(self, base_url: str, filename: str, image_bypass: Optional[torch.Tensor] = None,
                                   mask_bypass: Optional[torch.Tensor] = None, local_name: str = None):
            # If we have something at the bypass inputs use it
            if image_bypass is not None or mask_bypass is not None:
                if image_bypass is None:
                    # Just a mask
                    assert mask_bypass is not None, "This should not be possible if image_bypass is None"  # For mypy
                    image_bypass = torch.zeros(mask_bypass.shape + (3,), dtype=torch.float32, device="cpu")
                    logger.warning("ImageDownload: Returning an empty image")
                elif mask_bypass is None:
                    # This is ComfyUI behavior when we don't have transparency
                    mask_bypass = torch.zeros((64, 64), dtype=torch.float32, device="cpu").unsqueeze(0)
                    logger.warning("ImageDownload: Returning an empty mask")
                return (image_bypass, mask_bypass)

            save_dir = get_input_directory()
            dest_fname = local_name or filename
            local_filepath = os.path.join(save_dir, dest_fname)

            if not os.path.exists(local_filepath):
                logger.info(f"File '{filename}' not found locally. Attempting to download.")

                if not base_url.endswith('/'):
                    base_url += '/'
                download_url = base_url + filename

                try:
                    download_file(logger, url=download_url, save_dir=save_dir, file_name=dest_fname, kind="image")
                except Exception as e:
                    logger.error(f"Download failed for {download_url}: {e}", exc_info=True)
                    raise
            else:
                logger.info(f"Found existing file, skipping download: '{local_filepath}'")

            # --- REUSE ComfyUI's LoadImage LOGIC ---
            try:
                # Instantiate the built-in LoadImage node
                loader_instance = LoadImage()

                # The LoadImage node's `load_image` method expects the filename as passed
                # by the ComfyUI widget, which is just the filename. It internally
                # resolves the path using folder_paths.

                logger.debug(f"Calling built-in LoadImage.load_image() with filename: '{dest_fname}'")

                # Call the method and return its result directly
                result = loader_instance.load_image(dest_fname)
                # This information is for the preview, as we are an output node and we return images
                # they will be displayed in our node. Quite simple.
                downloaded_file = {
                     "images": [{
                         "filename": dest_fname,
                         "subfolder": "",
                         "type": "input"  # We stored the file in the "input" folder
                     }]
                }
                return {"ui": downloaded_file, "result": result}

            except Exception as e:
                logger.error(f"Failed to load image '{filename}' using built-in LoadImage node: {e}", exc_info=True)
                # Re-raise to make the error visible in ComfyUI
                raise IOError(f"Could not load the image file '{filename}' using the standard loader. "
                              "It may be corrupt or in an unsupported format.") from e
else:
    logger.error("Failed to import ComfyUI `LoadImage`, please fill an issue here: "
                 "https://github.com/set-soft/ComfyUI-ImageMisc/issues")


class CompositeFace:
    """
    A ComfyUI node to composite (paste) animated face crops back onto reference images.
    It handles a M-to-N relationship, where M reference images and bboxes correspond
    to M*N animated face images.
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "animated": ("IMAGE",),      # The M*N batch of cropped faces
                "reference": ("IMAGE",),     # The M batch of original context images
                "bboxes": ("BBOX",),         # The M list of (x, y, w, h) tuples
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("images",)
    FUNCTION = "composite"

    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = ("Inserts the `animated` face in the `reference` images at the `bboxes` coordinates.")
    UNIQUE_NAME = "SET_CompositeFace"
    DISPLAY_NAME = "Face Composite"

    def composite(self, animated: torch.Tensor, reference: torch.Tensor, bboxes: list):
        # 1. Get batch sizes and validate the M vs M*N relationship
        ref_count = reference.shape[0]
        anim_count = animated.shape[0]
        bbox_count = len(bboxes)

        if ref_count == 0 or anim_count == 0:
            logger.info("Warning: One of the input image batches is empty. Returning empty tensor.")
            return (torch.zeros((0, 1, 1, 3)),)

        if ref_count != bbox_count:
            raise ValueError(f"Mismatch: Received {ref_count} reference images but {bbox_count} bboxes. "
                             "These must be equal.")

        if anim_count % ref_count != 0:
            raise ValueError(f"Batch size mismatch: The 'animated' batch ({anim_count}) is not a multiple of the "
                             f"'reference' batch ({ref_count}).")

        # N: Number of animated frames per reference image
        n_frames_per_ref = anim_count // ref_count
        logger.info(f"Processing {ref_count} reference images, each with {n_frames_per_ref} animated frames.")

        output_images = []

        # 2. Iterate through each reference image and its corresponding bbox
        for i in range(ref_count):
            ref_tensor = reference[i]
            bbox = bboxes[i]

            # The bbox from your code is the area to be replaced.
            # Assuming it's in the format (x, y, width, height)
            try:
                x, y, w, h = map(int, bbox)
            except (ValueError, TypeError) as e:
                raise TypeError(f"Bbox item {i} has an invalid format: {bbox}. Expected (x, y, w, h). Error: {e}")

            # Convert the reference image to Pillow Image ONCE before the inner loop
            ref_pil = tensor_to_pil(ref_tensor)

            # 3. For this one reference, loop through its N animated frames
            for j in range(n_frames_per_ref):
                anim_index = i * n_frames_per_ref + j
                anim_tensor = animated[anim_index]

                # Convert the small animated face to Pillow Image
                anim_pil = tensor_to_pil(anim_tensor)

                # 4. Resize the animated face to fit the target bbox
                # Image.Resampling.LANCZOS is a high-quality resampling filter.
                resized_anim_face = anim_pil.resize((w, h), Image.Resampling.LANCZOS)

                # 5. Perform the paste operation.
                # It's CRITICAL to work on a copy of the reference image for each frame.
                pasted_image_pil = ref_pil.copy()
                pasted_image_pil.paste(resized_anim_face, (x, y))

                # 6. Convert the final image back to a tensor and add to the output list
                final_tensor = pil_to_tensor(pasted_image_pil)
                output_images.append(final_tensor)

        # 7. Stack all the generated images into a single batch tensor
        if not output_images:
            return (torch.zeros_like(reference),)  # Return something if all pastes failed

        final_batch = torch.stack(output_images)

        return (final_batch,)


class CompositeFaceFrameByFrame(CompositeFace):
    """
    A ComfyUI node to composite animated frames onto reference frames on a 1-to-1 basis.
    It expects the 'animated' and 'reference' batches to have the same number of frames.
    It uses the *first* bounding box from the 'bboxes' input for all frames.
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "animated": ("IMAGE",),      # The batch of cropped/processed frames
                "reference": ("IMAGE",),     # The batch of original frames
                "bboxes": ("BBOX",),         # A list of bboxes; only the first is used
            },
        }

    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = ("Inserts the `animated` face in the `reference` video at the `bboxes` coordinates.")
    UNIQUE_NAME = "SET_CompositeFaceFrameByFrame"
    DISPLAY_NAME = "Face Composite (frame by frame)"

    def composite(self, animated: torch.Tensor, reference: torch.Tensor, bboxes: list):
        # 1. Validate inputs
        anim_count = animated.shape[0]
        ref_count = reference.shape[0]

        if anim_count != ref_count:
            raise ValueError(f"Batch size mismatch: Received {anim_count} animated frames and {ref_count} reference "
                             "frames. They must be equal.")

        if not bboxes:
            raise ValueError("Bboxes input is empty. A bounding box is required.")

        # 2. Extract the single bounding box to be used for all frames
        if len(bboxes) > 1:
            logger.info(f"Warning: Received {len(bboxes)} bboxes. Using only the first one for all frames.")

        try:
            # Use the first bbox from the list
            x, y, w, h = map(int, bboxes[0])
            bbox_to_use = (x, y, w, h)
        except (ValueError, TypeError) as e:
            raise TypeError(f"The first bbox has an invalid format: {bboxes[0]}. Expected (x, y, w, h). Error: {e}")

        logger.info(f"Compositing {anim_count} frames using static bbox: {bbox_to_use}")

        output_images = []

        # 3. Loop through each frame in a 1-to-1 fashion
        for i in range(anim_count):
            ref_tensor = reference[i]
            anim_tensor = animated[i]

            # Convert tensors to Pillow Images
            ref_pil = tensor_to_pil(ref_tensor)
            anim_pil = tensor_to_pil(anim_tensor)

            # 4. Resize the animated face to fit the bbox
            # Image.Resampling.LANCZOS is a high-quality filter comparable to OpenCV's INTER_CUBIC/LANCZOS4
            resized_anim_face = anim_pil.resize((w, h), Image.Resampling.LANCZOS)

            # 5. Perform the paste operation
            # Pillow's paste is simpler. It handles coordinates and requires a copy.
            pasted_image_pil = ref_pil.copy()
            pasted_image_pil.paste(resized_anim_face, (x, y))

            # 6. Convert back to tensor and add to output list
            final_tensor = pil_to_tensor(pasted_image_pil)
            output_images.append(final_tensor)

        # 7. Stack all images into the final output batch
        final_batch = torch.stack(output_images)

        return (final_batch,)


class NormalizeToImageNetDataset():
    """
    A ComfyUI node to normalize the values to the mean/std of the ImageNet dataset
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
            },
        }
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "normalize"
    CATEGORY = BASE_CATEGORY + "/" + NORMALIZATION
    DESCRIPTION = ("Normalize the image to the ImageNet dataset")
    UNIQUE_NAME = "SET_NormalizeToImageNetDataset"
    DISPLAY_NAME = "Normalize Image to ImageNet"
    imagenet_normalize = None

    def normalize(self, image: torch.Tensor):
        return (TF.normalize(image.permute(0, 3, 1, 2),  # BHWC -> BCHW
                             mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225]).permute(0, 2, 3, 1),)  # BCHW -> BHWC


class NormalizeToRangeMinus05to05():
    """
    A ComfyUI node to normalize the values to the [-0.5, 0.5] range
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {"required": {"image": ("IMAGE",), }, }
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "normalize"
    CATEGORY = BASE_CATEGORY + "/" + NORMALIZATION
    DESCRIPTION = ("Normalize the image to [-0.5, 0.5]")
    UNIQUE_NAME = "SET_NormalizeToRangeMinus05to05"
    DISPLAY_NAME = "Normalize Image to [-0.5, 0.5]"
    imagenet_normalize = None

    def normalize(self, image: torch.Tensor):
        return (TF.normalize(image.permute(0, 3, 1, 2),  # BHWC -> BCHW
                             mean=[0.5, 0.5, 0.5],
                             std=[1.0, 1.0, 1.0]).permute(0, 2, 3, 1),)  # BCHW -> BHWC


class NormalizeToRangeMinus1to1():
    """
    A ComfyUI node to normalize the values to the [-1, 1] range
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {"required": {"image": ("IMAGE",), }, }
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "normalize"
    CATEGORY = BASE_CATEGORY + "/" + NORMALIZATION
    DESCRIPTION = ("Normalize the image to [-1, 1]")
    UNIQUE_NAME = "SET_NormalizeToRangeMinus1to1"
    DISPLAY_NAME = "Normalize Image to [-1, 1] (i.e. GAN)"
    imagenet_normalize = None

    def normalize(self, image: torch.Tensor):
        return (TF.normalize(image.permute(0, 3, 1, 2),  # BHWC -> BCHW
                             mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5]).permute(0, 2, 3, 1),)  # BCHW -> BHWC


class ApplyMaskAFFCE:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "images": ("IMAGE",),
                "masks": ("MASK",),
                "blur_size": BLUR_SIZE_OPT,
                "blur_size_two": BLUR_SIZE_TWO_OPT,
                "fill_color": ("BOOLEAN", {
                    "default": False,
                    "tooltip": ("Fill the background using a color.\n"
                                "Returns an RGB image, otherwise an RGBA.")
                }),
                "color": COLOR_OPT,
                "batched":  ("BOOLEAN", {
                    "default": True,
                    "tooltip": ("Process the images at once.\n"
                                "Faster, needs more memory")
                }),
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK",)
    RETURN_NAMES = ("image", "mask",)
    FUNCTION = "get_foreground"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = ("Apply a mask to an image using\n"
                   "Approximate Fast Foreground Colour Estimation.\n"
                   "https://github.com/Photoroom/fast-foreground-estimation")
    UNIQUE_NAME = "SET_ApplyMaskAFFCE"
    DISPLAY_NAME = "Apply Mask using AFFCE"

    def get_foreground(self, images, masks, blur_size=91, blur_size_two=7, fill_color=False, color=None, batched=True):
        out_images = apply_mask(logger, images, masks, model_management.get_torch_device(), blur_size, blur_size_two,
                                fill_color, color, batched)
        return out_images.cpu(), masks.cpu()


class AFFCE:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "images": ("IMAGE",),
                "masks": ("MASK",),
                "blur_size": BLUR_SIZE_OPT,
                "blur_size_two": BLUR_SIZE_TWO_OPT,
                "batched":  ("BOOLEAN", {
                    "default": True,
                    "tooltip": ("Process the images at once.\n"
                                "Faster, needs more memory")
                }),
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK",)
    RETURN_NAMES = ("foreground", "mask",)
    FUNCTION = "get_foreground"
    CATEGORY = BASE_CATEGORY + "/" + FOREGROUND
    DESCRIPTION = ("Estimate the foreground image using\n"
                   "Approximate Fast Foreground Colour Estimation.\n"
                   "https://github.com/Photoroom/fast-foreground-estimation")
    UNIQUE_NAME = "SET_AFFCE"
    DISPLAY_NAME = "Estimate foreground (AFFCE)"

    def get_foreground(self, images, masks, blur_size=91, blur_size_two=7, batched=True):
        device = model_management.get_torch_device()
        images_on_device = images.to(device)
        masks_on_device = masks.to(device)

        out_images = affce(images_on_device, masks_on_device, r1=blur_size, r2=blur_size_two, batched=batched)

        return out_images.cpu(), masks.cpu()


class FMLFE:
    """
    A ComfyUI node that uses the Fast Multi-Level Foreground Estimation algorithm
    to produce a high-quality foreground and background separation. It can
    intelligently select the best available backend (CuPy, OpenCL, Numba, or PyTorch).
    """

    @classmethod
    def INPUT_TYPES(cls):
        # Create the dropdown list for the implementation choice
        impl_list = ['auto'] + IMPL_PRIORITY

        return {
            "required": {
                "images": ("IMAGE", {
                    "tooltip": "The source image(s) from which to estimate the foreground and background."
                }),
                "masks": ("MASK", {
                    "tooltip": "The alpha matte that guides the estimation. White areas are treated as known "
                               "foreground, black as known background, and gray areas are the semi-transparent "
                               "regions the algorithm will solve for."
                }),
                "implementation": (impl_list, {
                    "default": "auto",
                    "tootip": "Select the computation backend. 'auto' mode will automatically try to use the "
                              "fastest available implementation, in order of priority: CuPy (NVIDIA GPU), "
                              "OpenCL (GPU), Numba (CPU/GPU), and finally the pure PyTorch version."
                }),
            },
            "optional": {
                "regularization": ("FLOAT", {
                    "default": 1e-5,
                    "min": 0.0,
                    "max": 0.1,
                    "step": 1e-5,
                    "display": "number",
                    "tooltip": "The regularization strength (epsilon). This acts as a smoothness prior. "
                               "Higher values result in smoother, more blended foreground and background colors, "
                               "but may lose very fine details. Lower values preserve more detail but can be noisier."
                }),
                "n_small_iterations": ("INT", {
                    "default": 10,
                    "min": 1,
                    "max": 100,
                    "tooltip": "The number of solver iterations to perform on the lower-resolution levels of the "
                               "image pyramid. More iterations can improve quality at the cost of speed."
                }),
                "n_big_iterations": ("INT", {
                    "default": 2,
                    "min": 1,
                    "max": 100,
                    "tooltip": "The number of solver iterations to perform on the higher-resolution (larger) levels "
                               "of the image pyramid. Fewer iterations are typically needed at high resolution as the "
                               "details are propagated up from the smaller levels."
                }),
                "small_size": ("INT", {
                    "default": 32,
                    "min": 8,
                    "max": 256,
                    "tooltip": "The pixel dimension threshold. Image pyramid levels smaller than this size will use "
                               "the higher 'n_small_iterations' count, while larger levels will use 'n_big_iterations'."
                }),
                "gradient_weight": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 10.0,
                    "step": 0.1,
                    "tooltip": "Controls how strongly the edges in the alpha matte influence color blending. "
                               "A higher value makes the algorithm respect the mask's edges more, leading to sharper "
                               "color boundaries. A lower value allows more color bleeding, an effect similar to "
                               "increasing regularization."
                }),
            }
        }

    RETURN_TYPES = ("IMAGE", "IMAGE", "MASK",)
    RETURN_NAMES = ("foreground", "background", "mask")
    FUNCTION = "estimate"
    CATEGORY = BASE_CATEGORY + "/" + FOREGROUND
    DESCRIPTION = ("Estimate the foreground image using\n"
                   "Fast Multi-Level Foreground Estimation.")
    UNIQUE_NAME = "SET_FMLFE"
    DISPLAY_NAME = "Estimate foreground (FMLFE)"

    def estimate(self, images: torch.Tensor, masks: torch.Tensor, implementation: str,
                 regularization: float, n_small_iterations: int, n_big_iterations: int,
                 small_size: int, gradient_weight: float):
        try:
            foregrounds, backgrounds = fmlfe(
                images=images,
                masks=masks,
                logger=logger,
                implementation=implementation,
                regularization=regularization,
                n_small_iterations=n_small_iterations,
                n_big_iterations=n_big_iterations,
                small_size=small_size,
                gradient_weight=gradient_weight
            )

            return (foregrounds, backgrounds, masks,)

        except Exception as e:
            # This ensures that if all backends fail, the error is clearly visible in the ComfyUI console.
            logger.error("Failed to execute ML Foreground Estimation. All backends failed.")
            logger.error(f"Last error: {e}")
            # Raising the exception will stop the workflow and show the error to the user.
            raise e


class CreateEmptyImage:
    """
    A ComfyUI node to create a solid-color image tensor.
    The output dimensions can be specified manually or inherited from an optional input image.
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "width": ("INT", {
                    "default": 1024,
                    "min": 1,
                    "max": 8192,
                    "step": 8,
                    "tooltip": "The width of the new image in pixels. This value is ignored if a `reference` is provided."
                }),
                "height": ("INT", {
                    "default": 1024,
                    "min": 1,
                    "max": 8192,
                    "step": 8,
                    "tooltip": "The height of the new image in pixels. This value is ignored if a `reference` is provided."
                }),
                "batch_size": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 64,
                    "tooltip": "The number of images to create in the batch. This value is ignored if a `reference` "
                               "is provided."
                }),
                "color": COLOR_OPT,
            },
            "optional": {
                "reference": ("IMAGE", {
                    "tooltip": "If an image is connected here, its dimensions (batch size, height, and width) will be "
                               "used for the new image, overriding the manual width, height, and batch_size inputs."
                }),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "create_image"
    CATEGORY = BASE_CATEGORY + "/generation"
    DESCRIPTION = ("Create a solid-color image.\n"
                   "If the optional image is provides uses its shape.")
    UNIQUE_NAME = "SET_CreateEmptyImage"
    DISPLAY_NAME = "Create Empty Image"

    def create_image(self, width: int, height: int, batch_size: int, color: str,
                     reference: Optional[torch.Tensor] = None):
        # --- 1. Determine the final shape of the output tensor ---
        if reference is not None:
            # If an image is provided, its shape overrides the manual inputs
            b, h, w, _ = reference.shape
        else:
            b, h, w = batch_size, height, width

        # --- 2. Parse the color string ---
        # The function returns a tuple of floats in the [0, 1] range
        rgb_color = color_to_rgb_float(logger, color)

        # --- 3. Create the tensor efficiently ---
        # Create a small color tensor and then expand it to the final size.
        # This is highly memory-efficient as it creates a view, not a full-size copy.
        # Tensors should be created on the CPU by default in generator nodes.
        color_tensor = torch.tensor(rgb_color, dtype=torch.float32, device="cpu").view(1, 1, 1, 3)
        final_image = color_tensor.expand(b, h, w, 3)

        return (final_image,)
