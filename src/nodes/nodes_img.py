# -*- coding: utf-8 -*-
# Copyright (c) 2025 Salvador E. Tropea
# Copyright (c) 2025 Instituto Nacional de TecnologÃ¯a Industrial
# License: GPL-3.0
# Project: ComfyUI-ImageMisc
# From code generated by Gemini 2.5 Pro
import numpy as np
import os
from PIL import Image  # Import the Python Imaging Library
from seconohe.apply_mask import apply_mask
from seconohe.downloader import download_file
# We are the main source, so we use the main_logger
from . import main_logger
import torch
import torchvision.transforms.functional as TF
from typing import Optional
try:
    from folder_paths import get_input_directory   # To get the ComfyUI input directory
    from comfy import model_management
except ModuleNotFoundError:
    # No ComfyUI, this is a test environment
    def get_input_directory():
        return ""
try:
    # We need to import the built-in LoadImage class for ImageDownload
    from nodes import LoadImage
    has_load_image = True
except Exception:
    has_load_image = False

logger = main_logger
BASE_CATEGORY = "image"
IO_CATEGORY = "io"
MANIPULATION_CATEGORY = "manipulation"
NORMALIZATION = "normalization"
BLUR_SIZE_OPT = ("INT", {"default": 90, "min": 1, "max": 255, "step": 1, })
BLUR_SIZE_TWO_OPT = ("INT", {"default": 6, "min": 1, "max": 255, "step": 1, })
COLOR_OPT = ("STRING", {
                "default": "#000000",
                "tooltip": "Color for fill.\n"
                           "Can be an hexadecimal (#RRGGBB).\n"
                           "Can comma separated RGB values in [0-255] or [0-1.0] range."})


def tensor_to_pil(tensor: torch.Tensor) -> Image.Image:
    """Converts a single image tensor (H, W, C) [0, 1] to a Pillow Image."""
    np_image = (tensor.cpu().numpy() * 255).astype(np.uint8)
    return Image.fromarray(np_image)


def pil_to_tensor(pil_image: Image.Image) -> torch.Tensor:
    """Converts a Pillow Image to a tensor (H, W, C) [0, 1]."""
    np_image = np.array(pil_image).astype(np.float32) / 255.0
    return torch.from_numpy(np_image)


if has_load_image:
    class ImageDownload:
        @classmethod
        def INPUT_TYPES(cls):
            return {
                "required": {
                    "base_url": ("STRING", {
                        "default":
                            "https://raw.githubusercontent.com/set-soft/AudioSeparation/refs/heads/main/example_workflows/",
                        "tooltip": "The base URL where the image file is located."
                    }),
                    "filename": ("STRING", {
                        "default": "audioseparation_logo.jpg",
                        "tooltip": "The name of the image file to download (e.g., photo.jpg, art.png)."
                    }),
                },
                "optional": {
                    "image_bypass": ("IMAGE", {
                         "tooltip": "If this image is present will be used instead of the downloaded one"
                    }),
                    "mask_bypass": ("MASK", {"tooltip": "If this mask is present will be used instead of the downloaded one"}),
                    "local_name": ("STRING", {
                        "default": "",
                        "tooltip": "The name used locally. Leave empty to use `filename`"
                    }),
                }
            }

        RETURN_TYPES = ("IMAGE", "MASK")
        RETURN_NAMES = ("image", "alpha_mask")
        FUNCTION = "load_or_download_image"
        CATEGORY = BASE_CATEGORY + "/" + IO_CATEGORY
        DESCRIPTION = ("Downloads an image to ComfyUI's 'input' directory if it doesn't exist, then loads it using the "
                       "built-in LoadImage logic.")
        UNIQUE_NAME = "SET_ImageDownload"
        DISPLAY_NAME = "Image Download and Load"
        # This node stores a result to disk. So this IS an output node.
        # It can be used without connecting any other node.
        # Declaring it as output helps with the preview mechanism.
        OUTPUT_NODE = True

        def load_or_download_image(self, base_url: str, filename: str, image_bypass: Optional[torch.Tensor] = None,
                                   mask_bypass: Optional[torch.Tensor] = None, local_name: str = None):
            # If we have something at the bypass inputs use it
            if image_bypass is not None or mask_bypass is not None:
                if image_bypass is None:
                    # Just a mask
                    assert mask_bypass is not None, "This should not be possible if image_bypass is None"  # For mypy
                    image_bypass = torch.zeros(mask_bypass.shape + (3,), dtype=torch.float32, device="cpu")
                    logger.warning("ImageDownload: Returning an empty image")
                elif mask_bypass is None:
                    # This is ComfyUI behavior when we don't have transparency
                    mask_bypass = torch.zeros((64, 64), dtype=torch.float32, device="cpu").unsqueeze(0)
                    logger.warning("ImageDownload: Returning an empty mask")
                return (image_bypass, mask_bypass)

            save_dir = get_input_directory()
            dest_fname = local_name or filename
            local_filepath = os.path.join(save_dir, dest_fname)

            if not os.path.exists(local_filepath):
                logger.info(f"File '{filename}' not found locally. Attempting to download.")

                if not base_url.endswith('/'):
                    base_url += '/'
                download_url = base_url + filename

                try:
                    download_file(logger, url=download_url, save_dir=save_dir, file_name=dest_fname, kind="image")
                except Exception as e:
                    logger.error(f"Download failed for {download_url}: {e}", exc_info=True)
                    raise
            else:
                logger.info(f"Found existing file, skipping download: '{local_filepath}'")

            # --- REUSE ComfyUI's LoadImage LOGIC ---
            try:
                # Instantiate the built-in LoadImage node
                loader_instance = LoadImage()

                # The LoadImage node's `load_image` method expects the filename as passed
                # by the ComfyUI widget, which is just the filename. It internally
                # resolves the path using folder_paths.

                logger.debug(f"Calling built-in LoadImage.load_image() with filename: '{filename}'")

                # Call the method and return its result directly
                result = loader_instance.load_image(filename)
                # This information is for the preview, as we are an output node and we return images
                # they will be displayed in our node. Quite simple.
                downloaded_file = {
                     "images": [{
                         "filename": filename,
                         "subfolder": "",
                         "type": "input"  # We stored the file in the "input" folder
                     }]
                }
                return {"ui": downloaded_file, "result": result}

            except Exception as e:
                logger.error(f"Failed to load image '{filename}' using built-in LoadImage node: {e}", exc_info=True)
                # Re-raise to make the error visible in ComfyUI
                raise IOError(f"Could not load the image file '{filename}' using the standard loader. "
                              "It may be corrupt or in an unsupported format.") from e
else:
    logger.error("Failed to import ComfyUI `LoadImage`, please fill an issue here: "
                 "https://github.com/set-soft/ComfyUI-ImageMisc/issues")


class CompositeFace:
    """
    A ComfyUI node to composite (paste) animated face crops back onto reference images.
    It handles a M-to-N relationship, where M reference images and bboxes correspond
    to M*N animated face images.
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "animated": ("IMAGE",),      # The M*N batch of cropped faces
                "reference": ("IMAGE",),     # The M batch of original context images
                "bboxes": ("BBOX",),         # The M list of (x, y, w, h) tuples
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("images",)
    FUNCTION = "composite"

    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = ("Inserts the `animated` face in the `reference` images at the `bboxes` coordinates.")
    UNIQUE_NAME = "SET_CompositeFace"
    DISPLAY_NAME = "Face Composite"

    def composite(self, animated: torch.Tensor, reference: torch.Tensor, bboxes: list):
        # 1. Get batch sizes and validate the M vs M*N relationship
        ref_count = reference.shape[0]
        anim_count = animated.shape[0]
        bbox_count = len(bboxes)

        if ref_count == 0 or anim_count == 0:
            logger.info("Warning: One of the input image batches is empty. Returning empty tensor.")
            return (torch.zeros((0, 1, 1, 3)),)

        if ref_count != bbox_count:
            raise ValueError(f"Mismatch: Received {ref_count} reference images but {bbox_count} bboxes. "
                             "These must be equal.")

        if anim_count % ref_count != 0:
            raise ValueError(f"Batch size mismatch: The 'animated' batch ({anim_count}) is not a multiple of the "
                             f"'reference' batch ({ref_count}).")

        # N: Number of animated frames per reference image
        n_frames_per_ref = anim_count // ref_count
        logger.info(f"Processing {ref_count} reference images, each with {n_frames_per_ref} animated frames.")

        output_images = []

        # 2. Iterate through each reference image and its corresponding bbox
        for i in range(ref_count):
            ref_tensor = reference[i]
            bbox = bboxes[i]

            # The bbox from your code is the area to be replaced.
            # Assuming it's in the format (x, y, width, height)
            try:
                x, y, w, h = map(int, bbox)
            except (ValueError, TypeError) as e:
                raise TypeError(f"Bbox item {i} has an invalid format: {bbox}. Expected (x, y, w, h). Error: {e}")

            # Convert the reference image to Pillow Image ONCE before the inner loop
            ref_pil = tensor_to_pil(ref_tensor)

            # 3. For this one reference, loop through its N animated frames
            for j in range(n_frames_per_ref):
                anim_index = i * n_frames_per_ref + j
                anim_tensor = animated[anim_index]

                # Convert the small animated face to Pillow Image
                anim_pil = tensor_to_pil(anim_tensor)

                # 4. Resize the animated face to fit the target bbox
                # Image.Resampling.LANCZOS is a high-quality resampling filter.
                resized_anim_face = anim_pil.resize((w, h), Image.Resampling.LANCZOS)

                # 5. Perform the paste operation.
                # It's CRITICAL to work on a copy of the reference image for each frame.
                pasted_image_pil = ref_pil.copy()
                pasted_image_pil.paste(resized_anim_face, (x, y))

                # 6. Convert the final image back to a tensor and add to the output list
                final_tensor = pil_to_tensor(pasted_image_pil)
                output_images.append(final_tensor)

        # 7. Stack all the generated images into a single batch tensor
        if not output_images:
            return (torch.zeros_like(reference),)  # Return something if all pastes failed

        final_batch = torch.stack(output_images)

        return (final_batch,)


class CompositeFaceFrameByFrame(CompositeFace):
    """
    A ComfyUI node to composite animated frames onto reference frames on a 1-to-1 basis.
    It expects the 'animated' and 'reference' batches to have the same number of frames.
    It uses the *first* bounding box from the 'bboxes' input for all frames.
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "animated": ("IMAGE",),      # The batch of cropped/processed frames
                "reference": ("IMAGE",),     # The batch of original frames
                "bboxes": ("BBOX",),         # A list of bboxes; only the first is used
            },
        }

    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = ("Inserts the `animated` face in the `reference` video at the `bboxes` coordinates.")
    UNIQUE_NAME = "SET_CompositeFaceFrameByFrame"
    DISPLAY_NAME = "Face Composite (frame by frame)"

    def composite(self, animated: torch.Tensor, reference: torch.Tensor, bboxes: list):
        # 1. Validate inputs
        anim_count = animated.shape[0]
        ref_count = reference.shape[0]

        if anim_count != ref_count:
            raise ValueError(f"Batch size mismatch: Received {anim_count} animated frames and {ref_count} reference "
                             "frames. They must be equal.")

        if not bboxes:
            raise ValueError("Bboxes input is empty. A bounding box is required.")

        # 2. Extract the single bounding box to be used for all frames
        if len(bboxes) > 1:
            logger.info(f"Warning: Received {len(bboxes)} bboxes. Using only the first one for all frames.")

        try:
            # Use the first bbox from the list
            x, y, w, h = map(int, bboxes[0])
            bbox_to_use = (x, y, w, h)
        except (ValueError, TypeError) as e:
            raise TypeError(f"The first bbox has an invalid format: {bboxes[0]}. Expected (x, y, w, h). Error: {e}")

        logger.info(f"Compositing {anim_count} frames using static bbox: {bbox_to_use}")

        output_images = []

        # 3. Loop through each frame in a 1-to-1 fashion
        for i in range(anim_count):
            ref_tensor = reference[i]
            anim_tensor = animated[i]

            # Convert tensors to Pillow Images
            ref_pil = tensor_to_pil(ref_tensor)
            anim_pil = tensor_to_pil(anim_tensor)

            # 4. Resize the animated face to fit the bbox
            # Image.Resampling.LANCZOS is a high-quality filter comparable to OpenCV's INTER_CUBIC/LANCZOS4
            resized_anim_face = anim_pil.resize((w, h), Image.Resampling.LANCZOS)

            # 5. Perform the paste operation
            # Pillow's paste is simpler. It handles coordinates and requires a copy.
            pasted_image_pil = ref_pil.copy()
            pasted_image_pil.paste(resized_anim_face, (x, y))

            # 6. Convert back to tensor and add to output list
            final_tensor = pil_to_tensor(pasted_image_pil)
            output_images.append(final_tensor)

        # 7. Stack all images into the final output batch
        final_batch = torch.stack(output_images)

        return (final_batch,)


class NormalizeToImageNetDataset():
    """
    A ComfyUI node to normalize the values to the mean/std of the ImageNet dataset
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
            },
        }
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "normalize"
    CATEGORY = BASE_CATEGORY + "/" + NORMALIZATION
    DESCRIPTION = ("Normalize the image to the ImageNet dataset")
    UNIQUE_NAME = "SET_NormalizeToImageNetDataset"
    DISPLAY_NAME = "Normalize Image to ImageNet"
    imagenet_normalize = None

    def normalize(self, image: torch.Tensor):
        return (TF.normalize(image.permute(0, 3, 1, 2),  # BHWC -> BCHW
                             mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225]).permute(0, 2, 3, 1),)  # BCHW -> BHWC


class NormalizeToRangeMinus05to05():
    """
    A ComfyUI node to normalize the values to the [-0.5, 0.5] range
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {"required": {"image": ("IMAGE",), }, }
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "normalize"
    CATEGORY = BASE_CATEGORY + "/" + NORMALIZATION
    DESCRIPTION = ("Normalize the image to [-0.5, 0.5]")
    UNIQUE_NAME = "SET_NormalizeToRangeMinus05to05"
    DISPLAY_NAME = "Normalize Image to [-0.5, 0.5]"
    imagenet_normalize = None

    def normalize(self, image: torch.Tensor):
        return (TF.normalize(image.permute(0, 3, 1, 2),  # BHWC -> BCHW
                             mean=[0.5, 0.5, 0.5],
                             std=[1.0, 1.0, 1.0]).permute(0, 2, 3, 1),)  # BCHW -> BHWC


class NormalizeToRangeMinus1to1():
    """
    A ComfyUI node to normalize the values to the [-1, 1] range
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {"required": {"image": ("IMAGE",), }, }
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "normalize"
    CATEGORY = BASE_CATEGORY + "/" + NORMALIZATION
    DESCRIPTION = ("Normalize the image to [-1, 1]")
    UNIQUE_NAME = "SET_NormalizeToRangeMinus1to1"
    DISPLAY_NAME = "Normalize Image to [-1, 1] (i.e. GAN)"
    imagenet_normalize = None

    def normalize(self, image: torch.Tensor):
        return (TF.normalize(image.permute(0, 3, 1, 2),  # BHWC -> BCHW
                             mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5]).permute(0, 2, 3, 1),)  # BCHW -> BHWC


class ApplyMaskAFFCE:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "images": ("IMAGE",),
                "masks": ("MASK",),
                "blur_size": BLUR_SIZE_OPT,
                "blur_size_two": BLUR_SIZE_TWO_OPT,
                "fill_color": ("BOOLEAN", {"default": False}),
                "color": COLOR_OPT,
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK",)
    RETURN_NAMES = ("image", "mask",)
    FUNCTION = "get_foreground"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = ("Apply a mask to an image using\n"
                   "Approximate Fast Foreground Colour Estimation.\n"
                   "https://github.com/Photoroom/fast-foreground-estimation")
    UNIQUE_NAME = "SET_ApplyMaskAFFCE"
    DISPLAY_NAME = "Apply Mask using AFFCE"

    def get_foreground(self, images, masks, blur_size=91, blur_size_two=7, fill_color=False, color=None):
        out_images = apply_mask(logger, images, masks, model_management.get_torch_device(), blur_size, blur_size_two,
                                fill_color, color)
        return out_images.cpu(), masks.cpu()
